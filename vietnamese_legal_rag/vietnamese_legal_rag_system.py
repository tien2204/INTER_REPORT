# Vietnamese Legal RAG System with TTS on Kaggle
# S·ª≠ d·ª•ng ViLM/VinAllama-2.7B-Chat + RAG + TTS

# ================================
# PH·∫¶N 1: C√ÄI ƒê·∫∂T TH∆Ø VI·ªÜN (Updated)
# ================================

# Ch·∫°y c√°c l·ªánh n√†y trong Kaggle notebook ho·∫∑c terminal
"""
!pip install -q transformers torch datasets
!pip install -q langchain langchain-community langchain-core
!pip install -q weaviate-client sentence-transformers rank_bm25
!pip install -q FlagEmbedding soundfile librosa pydub IPython
!pip install -q accelerate bitsandbytes
!pip install -q faiss-cpu  # Alternative vector search
"""

# ================================
# PH·∫¶N 2: IMPORT C√ÅC TH∆Ø VI·ªÜN
# ================================

import torch
import weaviate
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, 
    pipeline, BitsAndBytesConfig
)
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Weaviate
from langchain_core.embeddings import Embeddings
from langchain_core.language_models.llms import LLM
from langchain.chains import RetrievalQA
from langchain_core.documents import Document
from typing import List, Optional, Any
import warnings
warnings.filterwarnings('ignore')

# ================================
# PH·∫¶N 3: THI·∫æT L·∫¨P WEAVIATE
# ================================

class WeaviateSetup:
    def __init__(self):
        # S·ª≠ d·ª•ng Weaviate local/in-memory cho test
        try:
            # Th·ª≠ k·∫øt n·ªëi Weaviate local tr∆∞·ªõc
            self.client = weaviate.Client("http://localhost:8080")
            print("‚úÖ K·∫øt n·ªëi Weaviate local th√†nh c√¥ng")
        except:
            try:
                # Fallback: S·ª≠ d·ª•ng WCS (Weaviate Cloud Service) sandbox
                # Ho·∫∑c setup local Weaviate instance
                print("‚ö†Ô∏è Kh√¥ng th·ªÉ k·∫øt n·ªëi Weaviate local")
                print("üîß ƒêang setup Weaviate in-memory alternative...")
                self.client = self.setup_inmemory_alternative()
            except Exception as e:
                print(f"‚ùå L·ªói Weaviate: {e}")
                print("üîÑ S·ª≠ d·ª•ng alternative vector storage...")
                self.client = None
                
        if self.client:
            self.setup_schema()
    
    def setup_inmemory_alternative(self):
        # Alternative simple vector storage
        class SimpleVectorStore:
            def __init__(self):
                self.vectors = []
                self.documents = []
                
            def add_documents(self, docs_with_vectors):
                for doc, vector in docs_with_vectors:
                    self.documents.append(doc)
                    self.vectors.append(vector)
            
            def similarity_search(self, query_vector, k=5):
                if not self.vectors:
                    return []
                
                # Compute cosine similarity
                similarities = []
                query_norm = np.linalg.norm(query_vector)
                
                for i, vec in enumerate(self.vectors):
                    vec_norm = np.linalg.norm(vec)
                    if vec_norm > 0 and query_norm > 0:
                        sim = np.dot(query_vector, vec) / (query_norm * vec_norm)
                        similarities.append((sim, i))
                
                # Sort by similarity
                similarities.sort(key=lambda x: x[0], reverse=True)
                
                # Return top k
                results = []
                for sim, idx in similarities[:k]:
                    results.append(self.documents[idx])
                
                return results
        
        return SimpleVectorStore()
    
    def setup_schema(self):
        if hasattr(self.client, 'schema'):  # Real Weaviate
            # T·∫°o schema cho Vietnamese Legal documents
            schema = {
                "classes": [{
                    "class": "LegalDocument",
                    "description": "Vietnamese legal document",
                    "properties": [
                        {
                            "name": "content",
                            "dataType": ["text"],
                            "description": "Document content"
                        },
                        {
                            "name": "source",
                            "dataType": ["text"],
                            "description": "Document source"
                        },
                        {
                            "name": "chunk_id",
                            "dataType": ["int"],
                            "description": "Chunk identifier"
                        }
                    ]
                }]
            }
            
            # X√≥a schema c≈© n·∫øu t·ªìn t·∫°i
            try:
                if self.client.schema.exists("LegalDocument"):
                    self.client.schema.delete_class("LegalDocument")
            except:
                pass
            
            self.client.schema.create(schema)

# ================================
# PH·∫¶N 4: CUSTOM BGE-M3 EMBEDDINGS
# ================================

class BGEM3Embeddings(Embeddings):
    def __init__(self):
        self.model = SentenceTransformer('BAAI/bge-m3')
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self.model.encode(texts, normalize_embeddings=True).tolist()
    
    def embed_query(self, text: str) -> List[float]:
        return self.model.encode([text], normalize_embeddings=True)[0].tolist()

# ================================
# PH·∫¶N 5: CUSTOM VINALLAMA LLM
# ================================

class VinaLlamaLLM(LLM):
    """Custom LLM wrapper cho VinaLlama model"""
    
    def __init__(self, **kwargs):
        # Kh·ªüi t·∫°o parent class tr∆∞·ªõc
        super().__init__(**kwargs)
        
        print("ü§ñ ƒêang load VinaLlama model...")
        
        # C·∫•u h√¨nh quantization ƒë·ªÉ ti·∫øt ki·ªám memory
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        # Load model v√† tokenizer
        model_name = "vilm/vinallama-2.7b-chat"
        
        try:
            self._tokenizer = AutoTokenizer.from_pretrained(model_name)
            self._model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=quantization_config,
                device_map="auto",
                torch_dtype=torch.float16
            )
            
            # Thi·∫øt l·∫≠p pad token
            if self._tokenizer.pad_token is None:
                self._tokenizer.pad_token = self._tokenizer.eos_token
                
            print("‚úÖ VinaLlama model loaded successfully")
            
        except Exception as e:
            print(f"‚ùå L·ªói load model: {e}")
            print("üîÑ S·ª≠ d·ª•ng fallback model...")
            self._tokenizer = None
            self._model = None
    
    @property
    def _llm_type(self) -> str:
        return "vinallama"
    
    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs: Any) -> str:
        if self._model is None or self._tokenizer is None:
            return "Xin l·ªói, model ch∆∞a ƒë∆∞·ª£c load th√†nh c√¥ng. Vui l√≤ng ki·ªÉm tra l·∫°i c·∫•u h√¨nh."
        
        try:
            # Format prompt cho chat - simplified
            chat_prompt = f"H√£y tr·∫£ l·ªùi c√¢u h·ªèi sau d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p:\n\n{prompt}\n\nTr·∫£ l·ªùi ng·∫Øn g·ªçn:"
            
            # Tokenize v·ªõi padding v√† truncation
            inputs = self._tokenizer(
                chat_prompt, 
                return_tensors="pt", 
                max_length=1024,
                truncation=True,
                padding=True
            )
            
            # Move to device if available
            if torch.cuda.is_available():
                inputs = {k: v.to('cuda') for k, v in inputs.items()}
            
            # Generate v·ªõi c·∫£i thi·ªán parameters
            with torch.no_grad():
                outputs = self._model.generate(
                    **inputs,
                    max_new_tokens=256,  # Gi·∫£m ƒë·ªÉ tr√°nh l·∫∑p
                    temperature=0.3,     # Gi·∫£m temperature
                    do_sample=True,
                    repetition_penalty=1.2,  # Th√™m repetition penalty
                    no_repeat_ngram_size=3,  # Tr√°nh l·∫∑p ngram
                    pad_token_id=self._tokenizer.eos_token_id,
                    eos_token_id=self._tokenizer.eos_token_id,
                    early_stopping=True
                )
            
            # Decode response
            generated_ids = outputs[0][inputs['input_ids'].shape[1]:]  # Ch·ªâ l·∫•y ph·∫ßn generated
            response = self._tokenizer.decode(generated_ids, skip_special_tokens=True)
            
            # Clean up response
            response = response.strip()
            
            # Remove repetitive patterns
            lines = response.split('\n')
            cleaned_lines = []
            prev_line = ""
            
            for line in lines:
                line = line.strip()
                if line and line != prev_line and len(line) > 5:
                    cleaned_lines.append(line)
                    prev_line = line
                if len(cleaned_lines) >= 5:  # Gi·ªõi h·∫°n ƒë·ªô d√†i
                    break
            
            final_response = '\n'.join(cleaned_lines)
            
            return final_response if final_response else "Xin l·ªói, t√¥i kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi ph√π h·ª£p."
            
        except Exception as e:
            print(f"Generation error: {e}")
            return f"Xin l·ªói, ƒë√£ x·∫£y ra l·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi: {str(e)}"

# ================================
# PH·∫¶N 6: HYBRID RETRIEVAL (BM25 + BGE-M3)
# ================================

class HybridRetriever:
    def __init__(self, weaviate_client, embeddings):
        self.weaviate_client = weaviate_client
        self.embeddings = embeddings
        self.bm25 = None
        self.documents = []
        self.is_simple_store = not hasattr(weaviate_client, 'schema')
        
    def add_documents(self, documents: List[Document]):
        self.documents = documents
        
        # Chu·∫©n b·ªã BM25
        texts = [doc.page_content for doc in documents]
        tokenized_texts = [text.split() for text in texts]
        self.bm25 = BM25Okapi(tokenized_texts)
        
        # Th√™m v√†o vector store
        if self.is_simple_store:
            # Simple vector store
            docs_with_vectors = []
            for doc in documents:
                vector = self.embeddings.embed_query(doc.page_content)
                docs_with_vectors.append((doc, vector))
            self.weaviate_client.add_documents(docs_with_vectors)
        else:
            # Real Weaviate
            for i, doc in enumerate(documents):
                try:
                    self.weaviate_client.data_object.create(
                        data_object={
                            "content": doc.page_content,
                            "source": doc.metadata.get("source", "unknown"),
                            "chunk_id": i
                        },
                        class_name="LegalDocument",
                        vector=self.embeddings.embed_query(doc.page_content)
                    )
                except Exception as e:
                    print(f"Warning: Could not add document {i}: {e}")
    
    def retrieve(self, query: str, k: int = 5) -> List[Document]:
        retrieved_docs = []
        
        # BM25 retrieval
        if self.bm25:
            bm25_scores = self.bm25.get_scores(query.split())
            bm25_top_k = np.argsort(bm25_scores)[-k//2:][::-1]
            
            # Th√™m BM25 results
            for idx in bm25_top_k:
                if idx < len(self.documents):
                    doc = self.documents[idx]
                    doc.metadata["score"] = float(bm25_scores[idx])
                    doc.metadata["method"] = "bm25"
                    retrieved_docs.append(doc)
        
        # Vector retrieval
        if self.is_simple_store:
            # Simple vector search
            query_vector = self.embeddings.embed_query(query)
            vector_docs = self.weaviate_client.similarity_search(query_vector, k=k//2)
            for doc in vector_docs:
                doc.metadata["method"] = "vector"
                retrieved_docs.append(doc)
        else:
            # Real Weaviate search
            try:
                vector_results = self.weaviate_client.query\
                    .get("LegalDocument", ["content", "source", "chunk_id"])\
                    .with_near_vector({
                        "vector": self.embeddings.embed_query(query)
                    })\
                    .with_limit(k//2)\
                    .do()
                
                if "data" in vector_results and "Get" in vector_results["data"]:
                    for item in vector_results["data"]["Get"]["LegalDocument"]:
                        doc = Document(
                            page_content=item["content"],
                            metadata={
                                "source": item["source"],
                                "chunk_id": item["chunk_id"],
                                "method": "vector"
                            }
                        )
                        retrieved_docs.append(doc)
            except Exception as e:
                print(f"Warning: Vector search failed: {e}")
        
        # Remove duplicates v√† gi·ªõi h·∫°n k documents
        seen = set()
        unique_docs = []
        for doc in retrieved_docs:
            content_hash = hash(doc.page_content)
            if content_hash not in seen:
                seen.add(content_hash)
                unique_docs.append(doc)
                if len(unique_docs) >= k:
                    break
        
        return unique_docs[:k]

# ================================
# PH·∫¶N 7: VIETNAMESE TTS
# ================================

class VietnameseTTS:
    def __init__(self):
        # Alternative TTS implementation
        try:
            # Th·ª≠ s·ª≠ d·ª•ng ZaloPay TTS
            self.tts_pipeline = pipeline(
                "text-to-speech",
                model="zalopay/vietnamese-tts",
                device=0 if torch.cuda.is_available() else -1
            )
            self.tts_available = True
            print("‚úÖ TTS pipeline loaded successfully")
        except Exception as e:
            print(f"‚ö†Ô∏è TTS load failed: {e}")
            print("üîÑ Using alternative TTS method...")
            self.tts_available = False
    
    def synthesize(self, text: str, output_file: str = "output.wav"):
        if not self.tts_available:
            print("üìù TTS kh√¥ng kh·∫£ d·ª•ng, ch·ªâ hi·ªÉn th·ªã text")
            print(f"üéµ Text to speak: {text}")
            return None
            
        try:
            # Generate speech
            speech = self.tts_pipeline(text)
            
            # Save audio
            import soundfile as sf
            sf.write(output_file, speech["audio"], speech["sampling_rate"])
            
            return output_file
        except Exception as e:
            print(f"TTS Error: {e}")
            print(f"üìù Fallback - Text content: {text}")
            return None

# ================================
# PH·∫¶N 8: MAIN RAG SYSTEM
# ================================

class VietnameseLegalRAG:
    def __init__(self):
        print("üöÄ Kh·ªüi t·∫°o Vietnamese Legal RAG System...")
        
        # Setup components
        self.weaviate_setup = WeaviateSetup()
        self.embeddings = BGEM3Embeddings()
        self.llm = VinaLlamaLLM()
        self.retriever = HybridRetriever(
            self.weaviate_setup.client, 
            self.embeddings
        )
        self.tts = VietnameseTTS()
        
        # Load v√† process dataset
        self.load_legal_dataset()
        
        print("‚úÖ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng!")
    
    def load_legal_dataset(self):
        print("üìö ƒêang t·∫£i Vietnamese Law Corpus...")
        
        try:
            # Load dataset
            dataset = load_dataset("clapAI/vietnamese-law-corpus", split="train")
            
            # Convert to documents
            documents = []
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                separators=["\n\n", "\n", ".", "!", "?"]
            )
            
            for i, item in enumerate(dataset):
                # L·∫•y text content (ƒëi·ªÅu ch·ªânh theo structure c·ªßa dataset)
                text = item.get('text', '') or item.get('content', '') or str(item)
                
                if len(text.strip()) > 0:
                    # Split th√†nh chunks
                    chunks = text_splitter.split_text(text)
                    
                    for j, chunk in enumerate(chunks):
                        doc = Document(
                            page_content=chunk,
                            metadata={
                                "source": f"law_document_{i}",
                                "chunk": j,
                                "doc_id": i
                            }
                        )
                        documents.append(doc)
                
                # Gi·ªõi h·∫°n ƒë·ªÉ tr√°nh qu√° t·∫£i trong demo
                if i >= 100:  # Ch·ªâ l·∫•y 100 documents ƒë·∫ßu
                    break
            
            print(f"üìñ ƒê√£ x·ª≠ l√Ω {len(documents)} chunks t·ª´ {i+1} documents")
            
            # Add to retriever
            self.retriever.add_documents(documents)
            
        except Exception as e:
            print(f"‚ùå L·ªói khi t·∫£i dataset: {e}")
            print("üîÑ T·∫°o d·ªØ li·ªáu m·∫´u...")
            self.create_sample_data()
    
    def create_sample_data(self):
        # T·∫°o d·ªØ li·ªáu m·∫´u phong ph√∫ h∆°n v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam
        sample_texts = [
            "T√π chung th√¢n l√† h√¨nh ph·∫°t t√π kh√¥ng c√≥ th·ªùi h·∫°n, ƒë∆∞·ª£c √°p d·ª•ng cho nh·ªØng t·ªôi ph·∫°m ƒë·∫∑c bi·ªát nghi√™m tr·ªçng. Ng∆∞·ªùi b·ªã k·∫øt √°n t√π chung th√¢n c√≥ th·ªÉ ƒë∆∞·ª£c xem x√©t gi·∫£m √°n sau khi ch·∫•p h√†nh √≠t nh·∫•t 20 nƒÉm t√π.",
            
            "Lu·∫≠t D√¢n s·ª± quy ƒë·ªãnh v·ªÅ quy·ªÅn v√† nghƒ©a v·ª• c·ªßa c√¥ng d√¢n trong c√°c quan h·ªá d√¢n s·ª± nh∆∞ quy·ªÅn s·ªü h·ªØu, quy·ªÅn th·ª´a k·∫ø, h·ª£p ƒë·ªìng mua b√°n.",
            
            "B·ªô lu·∫≠t H√¨nh s·ª± Vi·ªát Nam quy ƒë·ªãnh c√°c t·ªôi ph·∫°m v√† h√¨nh ph·∫°t t∆∞∆°ng ·ª©ng. C√°c h√¨nh ph·∫°t ch√≠nh bao g·ªìm: c·∫£nh c√°o, ph·∫°t ti·ªÅn, c·∫£i t·∫°o kh√¥ng giam gi·ªØ, t√π c√≥ th·ªùi h·∫°n, t√π chung th√¢n v√† t·ª≠ h√¨nh.",
            
            "T√π c√≥ th·ªùi h·∫°n l√† h√¨nh ph·∫°t t·ª´ 3 th√°ng ƒë·∫øn 20 nƒÉm. ƒê·ªëi v·ªõi ng∆∞·ªùi ch∆∞a th√†nh ni√™n, th·ªùi h·∫°n t√π t·ªëi ƒëa l√† 12 nƒÉm. T√π chung th√¢n ch·ªâ √°p d·ª•ng cho ng∆∞·ªùi t·ª´ ƒë·ªß 18 tu·ªïi tr·ªü l√™n.",
            
            "Lu·∫≠t Lao ƒë·ªông b·∫£o v·ªá quy·ªÅn l·ª£i c·ªßa ng∆∞·ªùi lao ƒë·ªông bao g·ªìm quy·ªÅn ƒë∆∞·ª£c l√†m vi·ªác, quy·ªÅn ƒë∆∞·ª£c tr·∫£ l∆∞∆°ng c√¥ng b·∫±ng, quy·ªÅn ngh·ªâ ng∆°i, quy·ªÅn an to√†n lao ƒë·ªông.",
            
            "Hi·∫øn ph√°p l√† lu·∫≠t c∆° b·∫£n c·ªßa nh√† n∆∞·ªõc, quy ƒë·ªãnh v·ªÅ ch·∫ø ƒë·ªô ch√≠nh tr·ªã, quy·ªÅn v√† nghƒ©a v·ª• c∆° b·∫£n c·ªßa c√¥ng d√¢n, t·ªï ch·ª©c b·ªô m√°y nh√† n∆∞·ªõc.",
            
            "Lu·∫≠t H√¥n nh√¢n v√† Gia ƒë√¨nh quy ƒë·ªãnh v·ªÅ h√¥n nh√¢n, ly h√¥n, nu√¥i d∆∞·ª°ng con c√°i, b·∫£o v·ªá quy·ªÅn l·ª£i c·ªßa ph·ª• n·ªØ v√† tr·∫ª em trong gia ƒë√¨nh.",
            
            "C√°c t·ªôi ph·∫°m v·ªÅ tham nh≈©ng bao g·ªìm: nh·∫≠n h·ªëi l·ªô, ƒë∆∞a h·ªëi l·ªô, l·∫°m d·ª•ng ch·ª©c v·ª• quy·ªÅn h·∫°n, tham √¥ t√†i s·∫£n. H√¨nh ph·∫°t c√≥ th·ªÉ t·ª´ ph·∫°t ti·ªÅn ƒë·∫øn t√π chung th√¢n.",
            
            "Quy·ªÅn b√†o ch·ªØa l√† quy·ªÅn c∆° b·∫£n c·ªßa ng∆∞·ªùi b·ªã bu·ªôc t·ªôi. M·ªçi ng∆∞·ªùi ƒë·ªÅu c√≥ quy·ªÅn t·ª± b√†o ch·ªØa ho·∫∑c nh·ªù lu·∫≠t s∆∞ b√†o ch·ªØa khi b·ªã truy c·ª©u tr√°ch nhi·ªám h√¨nh s·ª±.",
            
            "T√≤a √°n nh√¢n d√¢n l√† c∆° quan x√©t x·ª≠ c·ªßa nh√† n∆∞·ªõc, c√≥ nhi·ªám v·ª• b·∫£o v·ªá c√¥ng l√Ω, b·∫£o v·ªá quy·ªÅn con ng∆∞·ªùi, quy·ªÅn c√¥ng d√¢n, ch·∫ø ƒë·ªô x√£ h·ªôi ch·ªß nghƒ©a."
        ]
        
        documents = []
        for i, text in enumerate(sample_texts):
            doc = Document(
                page_content=text,
                metadata={"source": f"sample_law_{i}", "doc_id": i}
            )
            documents.append(doc)
        
        self.retriever.add_documents(documents)
        print(f"‚úÖ ƒê√£ t·∫°o {len(sample_texts)} documents m·∫´u v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam")
    
    def query(self, question: str, return_audio: bool = True):
        print(f"‚ùì C√¢u h·ªèi: {question}")
        
        # Retrieve relevant documents
        print("üîç ƒêang t√¨m ki·∫øm t√†i li·ªáu li√™n quan...")
        relevant_docs = self.retriever.retrieve(question, k=3)
        
        # Debug: In ra documents ƒë∆∞·ª£c retrieve
        print(f"üìã T√¨m th·∫•y {len(relevant_docs)} t√†i li·ªáu li√™n quan:")
        for i, doc in enumerate(relevant_docs):
            print(f"  {i+1}. [{doc.metadata.get('method', 'unknown')}] {doc.page_content[:100]}...")
        
        # Create context
        context = "\n\n".join([doc.page_content for doc in relevant_docs])
        
        # Improved prompt v·ªõi context filtering
        if len(context.strip()) > 0:
            prompt = f"""Th√¥ng tin ph√°p lu·∫≠t li√™n quan:
{context}

C√¢u h·ªèi: {question}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin ph√°p lu·∫≠t tr√™n. N·∫øu th√¥ng tin kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi, h√£y n√≥i r√µ ƒëi·ªÅu ƒë√≥."""
        else:
            prompt = f"""C√¢u h·ªèi: {question}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam m·ªôt c√°ch ng·∫Øn g·ªçn v√† ch√≠nh x√°c."""
        
        # Generate response
        print("ü§ñ ƒêang t·∫°o c√¢u tr·∫£ l·ªùi...")
        response = self.llm._call(prompt)
        
        print(f"üìù C√¢u tr·∫£ l·ªùi: {response}")
        
        # Generate audio n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
        audio_file = None
        if return_audio:
            print("üîä ƒêang t·∫°o file √¢m thanh...")
            audio_file = self.tts.synthesize(response)
            if audio_file:
                print(f"üéµ ƒê√£ t·∫°o file √¢m thanh: {audio_file}")
        
        return {
            "question": question,
            "answer": response,
            "context": context,
            "relevant_docs": len(relevant_docs),
            "audio_file": audio_file,
            "retrieved_content": [doc.page_content for doc in relevant_docs]
        }

# ================================
# PH·∫¶N 9: CH·∫†Y DEMO
# ================================

def main():
    # Kh·ªüi t·∫°o h·ªá th·ªëng
    rag_system = VietnameseLegalRAG()
    
    # C√°c c√¢u h·ªèi demo
    demo_questions = [
        "Quy·ªÅn v√† nghƒ©a v·ª• c∆° b·∫£n c·ªßa c√¥ng d√¢n l√† g√¨?",
        "Lu·∫≠t h√¨nh s·ª± quy ƒë·ªãnh nh·ªØng t·ªôi ph·∫°m n√†o?",
        "T√π chung th√¢n l√† g√¨?",
        "H√¨nh ph·∫°t t√π c√≥ th·ªùi h·∫°n bao l√¢u?",
        "Lu·∫≠t h√¥n nh√¢n v√† gia ƒë√¨nh c√≥ nh·ªØng quy ƒë·ªãnh g√¨?"
    ]
    
    print("\n" + "="*50)
    print("üéØ DEMO VIETNAMESE LEGAL RAG SYSTEM")
    print("="*50)
    
    for i, question in enumerate(demo_questions, 1):
        print(f"\n--- Demo {i} ---")
        try:
            result = rag_system.query(question, return_audio=False)
            print("-" * 30)
        except Exception as e:
            print(f"‚ùå L·ªói trong demo {i}: {e}")
    
    # Interactive mode v·ªõi x·ª≠ l√Ω encoding
    print("\n" + "="*50)
    print("üí¨ CH·∫æ ƒê·ªò T√ÇM S·ª∞ PH√ÅP LU·∫¨T")
    print("Nh·∫≠p 'quit' ho·∫∑c 'tho√°t' ƒë·ªÉ tho√°t")
    print("Nh·∫≠p s·ªë ƒë·ªÉ ch·ªçn c√¢u h·ªèi m·∫´u:")
    for i, q in enumerate(demo_questions, 1):
        print(f"  {i}. {q}")
    print("="*50)
    
    while True:
        try:
            # Th·ª≠ nhi·ªÅu c√°ch nh·∫≠p input
            question = None
            
            # Method 1: Direct input v·ªõi encoding handling
            try:
                import sys
                if hasattr(sys.stdin, 'buffer'):
                    # ƒê·ªçc bytes v√† decode th·ªß c√¥ng
                    print("\n‚ùì C√¢u h·ªèi c·ªßa b·∫°n: ", end='', flush=True)
                    line = sys.stdin.buffer.readline()
                    question = line.decode('utf-8', errors='ignore').strip()
                else:
                    question = input("\n‚ùì C√¢u h·ªèi c·ªßa b·∫°n: ").strip()
            except UnicodeDecodeError:
                # Method 2: Fallback input
                print("\n‚ö†Ô∏è L·ªói encoding. Vui l√≤ng nh·∫≠p l·∫°i ho·∫∑c ch·ªçn s·ªë (1-5):")
                question = input("Input: ").strip()
            
            # X·ª≠ l√Ω input
            if not question:
                continue
                
            # Check exit commands
            if question.lower() in ['quit', 'exit', 'thoat', 'tho√°t', 'q']:
                break
            
            # Check if input is a number (choose from demo questions)
            try:
                num = int(question)
                if 1 <= num <= len(demo_questions):
                    question = demo_questions[num - 1]
                    print(f"üìù ƒê√£ ch·ªçn: {question}")
                else:
                    print("‚ùå S·ªë kh√¥ng h·ª£p l·ªá. Vui l√≤ng ch·ªçn t·ª´ 1-5")
                    continue
            except ValueError:
                # Not a number, treat as regular question
                pass
            
            # Process question
            if question.strip():
                try:
                    result = rag_system.query(question)
                    
                    # Play audio n·∫øu c√≥
                    if result["audio_file"]:
                        try:
                            from IPython.display import Audio, display
                            display(Audio(result["audio_file"]))
                        except:
                            print("‚ö†Ô∏è Kh√¥ng th·ªÉ ph√°t √¢m thanh trong m√¥i tr∆∞·ªùng n√†y")
                            
                except Exception as e:
                    print(f"‚ùå L·ªói x·ª≠ l√Ω c√¢u h·ªèi: {e}")
                    print("üîÑ Vui l√≤ng th·ª≠ l·∫°i v·ªõi c√¢u h·ªèi kh√°c")
                    
        except KeyboardInterrupt:
            print("\n\nüëã ƒê√£ d·ª´ng ch∆∞∆°ng tr√¨nh")
            break
        except Exception as e:
            print(f"\n‚ùå L·ªói kh√¥ng mong mu·ªën: {e}")
            print("üîÑ Ti·∫øp t·ª•c...")
    
    print("\nüëã C·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng h·ªá th·ªëng!")

# ================================
# TEST FUNCTION - Th√™m function test ri√™ng
# ================================

def test_system():
    """Function ƒë·ªÉ test h·ªá th·ªëng v·ªõi c√¢u h·ªèi c·ªë ƒë·ªãnh"""
    print("üß™ CH·∫†Y TEST H·ªÜ TH·ªêNG")
    print("="*50)
    
    try:
        rag_system = VietnameseLegalRAG()
        
        test_questions = [
            "T√π chung th√¢n l√† g√¨?",
            "Lu·∫≠t d√¢n s·ª± quy ƒë·ªãnh g√¨?",
            "H√¨nh ph·∫°t t√π c√≥ m·∫•y lo·∫°i?",
            "Quy·ªÅn b√†o ch·ªØa l√† g√¨?",
            "T√≤a √°n c√≥ ch·ª©c nƒÉng g√¨?"
        ]
        
        for i, question in enumerate(test_questions, 1):
            print(f"\nüîç Test {i}: {question}")
            print("-" * 40)
            
            try:
                result = rag_system.query(question, return_audio=False)
                print(f"‚úÖ Test {i} ho√†n th√†nh")
            except Exception as e:
                print(f"‚ùå Test {i} failed: {e}")
            
            print("=" * 40)
            
    except Exception as e:
        print(f"‚ùå L·ªói kh·ªüi t·∫°o h·ªá th·ªëng: {e}")

# ================================
# CH·∫†Y CH∆Ø∆†NG TR√åNH
# ================================

if __name__ == "__main__":
    import sys
    
    # Check command line arguments
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        test_system()
    else:
        try:
            main()
        except Exception as e:
            print(f"‚ùå L·ªói ch∆∞∆°ng tr√¨nh ch√≠nh: {e}")
            print("üîÑ Th·ª≠ ch·∫°y ch·∫ø ƒë·ªô test: python vietnamese_legal_rag_system.py test")

# ================================
# H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG TR√äN KAGGLE
# ================================

"""
C√ÅCH CH·∫†Y TR√äN KAGGLE:

1. T·∫°o m·ªôt Kaggle Notebook m·ªõi
2. B·∫≠t GPU (Settings -> Accelerator -> GPU)
3. Copy to√†n b·ªô code n√†y v√†o notebook
4. Uncomment v√† ch·∫°y ph·∫ßn c√†i ƒë·∫∑t th∆∞ vi·ªán ·ªü ƒë·∫ßu
5. Ch·∫°y c√°c cell theo th·ª© t·ª±
6. H·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông:
   - T·∫£i Vietnamese Law Corpus
   - Thi·∫øt l·∫≠p Weaviate embedded
   - Load VinaLlama model v·ªõi quantization
   - T·∫°o hybrid retriever (BM25 + BGE-M3)
   - S·∫µn s√†ng tr·∫£ l·ªùi c√¢u h·ªèi ph√°p lu·∫≠t

L∆ØU √ù:
- Code ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u cho environment Kaggle
- S·ª≠ d·ª•ng quantization ƒë·ªÉ ti·∫øt ki·ªám memory
- Embedded Weaviate kh√¥ng c·∫ßn setup ph·ª©c t·∫°p
- TTS c√≥ th·ªÉ kh√¥ng ho·∫°t ƒë·ªông perfect tr√™n Kaggle
- C√≥ th·ªÉ ƒëi·ªÅu ch·ªânh chunk_size v√† k ƒë·ªÉ t·ªëi ∆∞u performance

T√çNH NƒÇNG:
‚úÖ LLM: VinaLlama 2.7B Chat v·ªõi quantization
‚úÖ RAG: Hybrid BM25 + BGE-M3 embeddings
‚úÖ Vector Store: Weaviate v·ªõi fallback SimpleVectorStore
‚úÖ Dataset: Vietnamese Law Corpus t·ª´ HuggingFace
‚úÖ TTS: Vietnamese text-to-speech
‚úÖ Interactive Demo: Console interface v·ªõi menu
‚úÖ Error Handling: Robust error handling v√† fallback
‚úÖ Memory Optimization: 4-bit quantization cho large models
"""